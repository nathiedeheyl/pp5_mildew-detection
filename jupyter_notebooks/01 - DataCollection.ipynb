{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection and Preparation Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fetch data from Kaggle and prepare it.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Kaggle JSON file as authentication token\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate Dataset: inputs/mildew_dataset\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* No comments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.26.1 (from -r requirements.txt (line 1))\n",
            "  Using cached numpy-1.26.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
            "Collecting pandas==2.1.1 (from -r requirements.txt (line 2))\n",
            "  Using cached pandas-2.1.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (18 kB)\n",
            "Collecting matplotlib==3.8.0 (from -r requirements.txt (line 3))\n",
            "  Using cached matplotlib-3.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
            "Collecting seaborn==0.13.2 (from -r requirements.txt (line 4))\n",
            "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting plotly==5.17.0 (from -r requirements.txt (line 5))\n",
            "  Using cached plotly-5.17.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting Pillow==10.0.1 (from -r requirements.txt (line 6))\n",
            "  Using cached Pillow-10.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.5 kB)\n",
            "Collecting streamlit==1.40.2 (from -r requirements.txt (line 7))\n",
            "  Using cached streamlit-1.40.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting joblib==1.4.2 (from -r requirements.txt (line 8))\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting scikit-learn==1.3.1 (from -r requirements.txt (line 9))\n",
            "  Using cached scikit_learn-1.3.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-cpu==2.16.1 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-cpu==2.16.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/nathalievonheyl/Documents/ci-code_institute/PP5/pp5_mildew-detection/jupyter_notebooks'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/nathalievonheyl/Documents/ci-code_institute/PP5/pp5_mildew-detection'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Install and download Kaggle data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.7.4.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting bleach (from kaggle)\n",
            "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting certifi>=14.05.14 (from kaggle)\n",
            "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting charset-normalizer (from kaggle)\n",
            "  Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
            "Collecting idna (from kaggle)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting protobuf (from kaggle)\n",
            "  Downloading protobuf-6.30.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/nathalievonheyl/Documents/ci-code_institute/PP5/virtual_envs/mildew_venv/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
            "Collecting python-slugify (from kaggle)\n",
            "  Using cached python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting requests (from kaggle)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting setuptools>=21.0.0 (from kaggle)\n",
            "  Downloading setuptools-76.1.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: six>=1.10 in /Users/nathalievonheyl/Documents/ci-code_institute/PP5/virtual_envs/mildew_venv/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
            "Collecting text-unidecode (from kaggle)\n",
            "  Using cached text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tqdm (from kaggle)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting urllib3>=1.15.1 (from kaggle)\n",
            "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting webencodings (from kaggle)\n",
            "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading kaggle-1.7.4.2-py3-none-any.whl (173 kB)\n",
            "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Downloading setuptools-76.1.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Using cached bleach-6.2.0-py3-none-any.whl (163 kB)\n",
            "Using cached charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading protobuf-6.30.1-cp39-abi3-macosx_10_9_universal2.whl (417 kB)\n",
            "Using cached python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
            "Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: webencodings, text-unidecode, urllib3, tqdm, setuptools, python-slugify, protobuf, idna, charset-normalizer, certifi, bleach, requests, kaggle\n",
            "Successfully installed bleach-6.2.0 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 kaggle-1.7.4.2 protobuf-6.30.1 python-slugify-8.0.4 requests-2.32.3 setuptools-76.1.0 text-unidecode-1.3 tqdm-4.67.1 urllib3-2.3.0 webencodings-0.5.1\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change kaggle configuration directory to current working directory and permission of kaggle authentication json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download Kaggle dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/codeinstitute/cherry-leaves\n",
            "License(s): unknown\n"
          ]
        }
      ],
      "source": [
        "KaggleDatasetPath = \"codeinstitute/cherry-leaves\"\n",
        "DestinationFolder = \"inputs/mildew_dataset\"\n",
        "! kaggle datasets download -d {KaggleDatasetPath} -p {DestinationFolder}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = os.path.join(DestinationFolder, \"cherry-leaves.zip\")\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "  zip_ref.extractall(DestinationFolder)\n",
        "\n",
        "os.remove(zip_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Data Cleaning and Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning: Remove non-image files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the dynamic file path construction, I am using `os.path.join()` for easier handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_image_file(my_data_dir):\n",
        "  \"\"\"\n",
        "  Function to remove non-image files\n",
        "  from the dataset directory.\n",
        "  \"\"\"\n",
        "  image_extension = ('.png', '.jpg', '.jpeg')\n",
        "  folders = os.listdir(my_data_dir)  \n",
        "  for folder in folders:\n",
        "    folder_path =  os.path.join(my_data_dir, folder)\n",
        "\n",
        "    if not os.path.isdir(folder_path):\n",
        "      continue\n",
        "\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    i = []\n",
        "    j = []\n",
        "    for given_file in files:\n",
        "      if not given_file.lower().endswith(image_extension):\n",
        "        file_location = os.path.join(my_data_dir, folder, given_file)\n",
        "        os.remove(file_location)\n",
        "        i.append(1)\n",
        "      else:\n",
        "        j.append(1)\n",
        "        pass\n",
        "    print(f\"Folder: {folder} - has image file\", len(j))\n",
        "    print(f\"Folder: {folder} - has non-image file\", len(i))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From previous experience, I discovered that the macOS Finder creates `.DS_Store` files in every folder. These files are not actually folders, but `os.listdir()` can mistakenly include them in the list of folders. This then can cause errors when trying to further process the directories as class labels in image classification. \n",
        "\n",
        "I included an if/else statement to check whether an item is a real directory using `os.path.isdir()`. `.DS_Store` files are then already skipped in this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder: powdery_mildew - has image file 2104\n",
            "Folder: powdery_mildew - has non-image file 0\n",
            "Folder: healthy - has image file 2104\n",
            "Folder: healthy - has non-image file 0\n"
          ]
        }
      ],
      "source": [
        "remove_non_image_file(my_data_dir='inputs/mildew_dataset/cherry-leaves')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split data into train and validation, then test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_dataset_images(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio):\n",
        "  \"\"\"\n",
        "  Function to split images into train, validation, and test sets.\n",
        "\n",
        "  The function assumes `my_data_dir` contains folders representing class labels.\n",
        "  \n",
        "  Checks if the sum of all ratios equals 1.\n",
        "  Checks if dataset is already split by looking for 'test' folder.\n",
        "  Creates 'train', 'validation', 'test' directories if they do not yet exist.\n",
        "  Shuffles and distributes images based on ratio passed by parameters.\n",
        "  Moves images into their respective dataset folders.\n",
        "  Deletes original class folders after images have been moved.\n",
        "  \"\"\"\n",
        "  # make sure ratio of datasets sum is 1\n",
        "  if train_set_ratio + validation_set_ratio + test_set_ratio != 1.0:\n",
        "        print(\"train_set_ratio + validation_set_ratio + test_set_ratio should sum to 1.0\")\n",
        "        return\n",
        "  \n",
        "  # get list of labels (folders)\n",
        "  labels = os.listdir(my_data_dir)\n",
        "\n",
        "  # check if datasets have already been split\n",
        "  # check for test folder as an example\n",
        "  # exit function to avoid unnecessary processing\n",
        "  if os.path.exists(os.path.join(my_data_dir, 'test')):\n",
        "      print(\"Datasets have already been split.\")\n",
        "      return\n",
        "  \n",
        "  #create train, validation, and test folder with subfolders \"healthy\" and \"powdery_mildew\"\n",
        "  else:\n",
        "      for folder in ['train', 'validation', 'test']:\n",
        "          for label in labels:\n",
        "              os.makedirs(os.path.join(my_data_dir, folder, label))\n",
        "\n",
        "      # looping through labels\n",
        "      for label in labels:\n",
        "          \n",
        "          files = os.listdir(my_data_dir + '/' + label)\n",
        "          random.shuffle(files)\n",
        "\n",
        "          # calculate number of files for train and validation split according to ratio\n",
        "          # remaining images go into test set\n",
        "          train_set_files_qty = int(len(files) * train_set_ratio)\n",
        "          validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
        "\n",
        "          # move img data into new folders\n",
        "          count = 1\n",
        "          for file_name in files:\n",
        "              if count <= train_set_files_qty:\n",
        "                  shutil.move(os.path.join(my_data_dir, label, file_name),\n",
        "                              os.path.join(my_data_dir, 'train', label, file_name))\n",
        "\n",
        "              elif count <= (train_set_files_qty + validation_set_files_qty):\n",
        "                  shutil.move(os.path.join(my_data_dir, label, file_name),\n",
        "                              os.path.join(my_data_dir, 'validation', label, file_name))\n",
        "                  \n",
        "              else:\n",
        "                  shutil.move(os.path.join(my_data_dir, label, file_name),\n",
        "                              os.path.join(my_data_dir, 'test', label, file_name))\n",
        "              \n",
        "              count += 1\n",
        "          \n",
        "          # delete original category folders after moving files\n",
        "          os.rmdir(os.path.join(my_data_dir, label))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note to self: \n",
        "\n",
        "If any other hidden files similar to `.DS_Store` left in folder(s) `os.rmdir()` fill fail according to (https://docs.python.org/3/library/os.html#os.rmdir)\n",
        "stackoverflow recommends `shutil.rmtree()` (https://stackoverflow.com/a/62244641/28803519)\n",
        "\n",
        "Do more research and possibly refactor later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_dataset_images(my_data_dir=f\"inputs/mildew_dataset/cherry-leaves\",\n",
        "                     train_set_ratio=0.7,\n",
        "                     validation_set_ratio=0.1,\n",
        "                     test_set_ratio=0.2\n",
        "                     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusion and next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "The original img dataset of \"healthy\" and \"powdery_mildew\" has been organised into 'train', 'validation', and 'test' folders with the respective 'healthy' and 'powdery_mildew' subfolders.\n",
        "\n",
        "The `remove_non_image_file` and `split_dataset_images` functions ensure a clean strucutre, removing non-image files, skipping hidden files that would be mistaken as folders (interpreted as class labels), and avoid manual errors by using os methods.\n",
        "\n",
        "The original unstructured img dataset has been succesfully removed. The split datasets are ready for visualisation.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "The structured img data will be manipulated and plotted to understand and verify the dataset. Most relevant is to determine whether the classes of the dataset are balanced. In a next step I will also verify once more and check for incorrect labels (e.g. hidden files mistaken for labels). The goal of the next step will be to align the img data with the given business requirements."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "mildew_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
